# Projet de Data Interpreter IA

Ce projet est une application REST qui permet de traiter divers types de fichiers (Ã  savoir .xls, .xlsx, .csv, .json, .pdf, .py) et de gÃ©nÃ©rer des analyses sur ces fichiers en utilisant un modÃ¨le de langage large (LLM). Il intÃ¨gre des fonctionnalitÃ©s d'extraction de texte, d'images, de donnÃ©es relationnelles et de code Python. Il utilise l'Ã©cosystÃ¨me LangChain, des bases de donnÃ©es DuckDB, ainsi que FastAPI pour l'interface utilisateur.

## Table des matiÃ¨res

- [Quick Start](#quick-start)
- [Architecture du systÃ¨me](#architecture-du-systÃ¨me)
- [PrÃ©requis](#prÃ©requis)
- [Installation du Projet](#installation-du-projet)
- [Lancer l'Application](#lancer-lapplication)
- [Utilisation de l'API REST](#utilisation-de-lapi-rest)
- [FonctionnalitÃ©s ClÃ©s](#fonctionnalitÃ©s-clÃ©s)
- [Organisation des Fichiers](#organisation-des-fichiers)
- [Exemples de Fichiers SupportÃ©s](#exemples-de-fichiers-supportÃ©s)
- [Exemples d'usage avancÃ©s](#exemples-dusage-avancÃ©s)
- [Configuration de l'environnement](#configuration-de-lenvironnement)
- [Lancement Interface OpenWebui](#lancement-interface-openwebui)
- [FAQ](#faq)
- [Avertissement](#avertissement)

## Quick Start

### DÃ©marrage rapide en 5 minutes

1. **Clonez et installez les dÃ©pendances :**
   ```bash
   git clone <URL_DU_PROJET>
   cd <nom_du_rÃ©pertoire>
   pip install -r requirements.txt
   ```

2. **Installez Ollama et les modÃ¨les :**
   ```bash
   # Installation Ollama
   curl -o- https://ollama.com/download.sh | bash
   
   # DÃ©marrage du service
   ollama serve
   
   # TÃ©lÃ©chargement des modÃ¨les (dans un autre terminal)
   ollama pull duckdb-nsql:latest
   ollama pull mistral-small3.1:latest
   ```

3. **Lancez l'application :**
   ```bash
   # Mode Terminal
   sh run_terminal.sh ./data
   
   # OU Mode OpenWebUI
   sh run_pipelines.sh
   ```

4. **Testez avec un fichier :**
   ```bash
   curl -X POST "http://localhost:8000/query/" \
        -H "Content-Type: application/json" \
        -d '{"complex_query": "Analyse les donnÃ©es et donne-moi un rÃ©sumÃ©"}'
   ```

## Architecture du systÃ¨me

### Vue d'ensemble de l'architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Interface     â”‚    â”‚   Data           â”‚    â”‚   LLM Models    â”‚
â”‚   - OpenWebUI   â”‚â—„â”€â”€â–ºâ”‚   Interpreter    â”‚â—„â”€â”€â–ºâ”‚   - Ollama      â”‚
â”‚   - REST API    â”‚    â”‚   Core           â”‚    â”‚   - Mistral     â”‚
â”‚   - Terminal    â”‚    â”‚                  â”‚    â”‚   - DuckDB-NSQL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   File          â”‚    â”‚   Processing     â”‚    â”‚   Database      â”‚
â”‚   Processing    â”‚    â”‚   Pipeline       â”‚    â”‚   - DuckDB      â”‚
â”‚   - PDF/OCR     â”‚â—„â”€â”€â–ºâ”‚   - SQL Gen      â”‚â—„â”€â”€â–ºâ”‚   - SQLite      â”‚
â”‚   - Excel/CSV   â”‚    â”‚   - Python Gen   â”‚    â”‚   - History     â”‚
â”‚   - JSON/Python â”‚    â”‚   - Analysis     â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de traitement des donnÃ©es

1. **Ingestion** : Upload et dÃ©tection automatique du type de fichier
2. **Extraction** : Parsing spÃ©cialisÃ© selon le format (PDFâ†’OCR, Excelâ†’Tables, etc.)
3. **Stockage** : Chargement dans DuckDB avec schÃ©ma optimisÃ©
4. **Planification** : GÃ©nÃ©ration d'un plan d'analyse par le LLM
5. **ExÃ©cution** : GÃ©nÃ©ration et exÃ©cution de requÃªtes SQL/Python
6. **SynthÃ¨se** : Analyse des rÃ©sultats et gÃ©nÃ©ration de la rÃ©ponse finale

### PrÃ©sentation de l'algorithme du Data Interpreter sous forme de schÃ©ma

![SchÃ©ma de l'algorithme](./assets/data_interpreter_explain.png)

## PrÃ©requis

- Python 3.11 ou plus rÃ©cent
- `tesseract` pour l'extraction OCR
- Librairies Python listÃ©es dans `requirements.txt`
- `ollama` pour utiliser les LLM

### Installation de Tesseract

Ubuntu/Debian :
```bash
sudo apt install tesseract-ocr
```

macOS :
```bash
brew install tesseract
```

### Installation de Ollama

Pour utiliser les modÃ¨les LLM avec Ollama, suivez les instructions ci-dessous :

1. Installez Ollama :

    macOS :
    ```bash
    brew install ollama
    ```

    Linux :
    ```bash
    curl -o- https://ollama.com/download.sh | bash
    ```

2. Lancez le service Ollama :

    ```bash
    ollama serve
    ```

3. TÃ©lÃ©chargez les modÃ¨les LLM requis :

    ```bash
    ollama pull duckdb-nsql:latest
    ollama pull llama3.2:latest
    ollama pull mistral-small3.1:latest
    ```

## Installation du Projet

1. Clonez le rÃ©pertoire du projet :

    ```bash
    git clone <URL_DU_PROJET>
    cd <nom_du_rÃ©pertoire>
    ```

2. Installez les dÃ©pendances nÃ©cessaires Ã  l'aide de `requirements.txt` :

    ```bash
    pip install -r requirements.txt
    ```

## Lancer l'Application

Pour exÃ©cuter l'application, lancez la commande suivante :

```bash
sh run_terminal.sh <chemin_vers_vos_fichiers>
```

- Remplacez `<chemin_vers_vos_fichiers>` par le chemin des fichiers que vous souhaitez traiter.

L'application se lancera sur `http://0.0.0.0:8000`.

## Pour obtenir des indications de lancement dans le terminal


```bash
sh run_terminal.sh --help
```

## Utilisation de l'API REST

L'API REST est dÃ©veloppÃ©e avec FastAPI. Voici un exemple d'utilisation :

- Endpoint : `/query/`
- MÃ©thode : `POST`
- Corps de la requÃªte :
  ```json
  {
    "complex_query": "<votre_question>"
  }
  ```
- RÃ©ponse :
  ```json
  {
    "analysis_result": "<rÃ©sultat_de_l'analyse>"
  }
  ```

Vous pouvez tester l'API Ã  l'aide d'un outil comme `Postman` ou `curl`.

### Exemple d'ExÃ©cution via Curl

```bash
curl -X POST "http://localhost:8000/query/" -H "Content-Type: application/json" -d '{"complex_query": "Donne-moi les statistiques de ventes"}'
```

## FonctionnalitÃ©s ClÃ©s

1. **Extraction de Texte et Images des PDF :** Le projet utilise `pdfminer.six` pour extraire le texte et `PyMuPDF` pour extraire les images des fichiers PDF.
2. **Extraction de Texte par OCR :** `pytesseract` est utilisÃ© pour extraire le texte des images prÃ©sentes dans les PDF.
3. **Analyse de Code Python :** Extraction des fonctions, classes, imports et autres Ã©lÃ©ments d'un fichier `.py` en utilisant le module `ast`.
4. **Chargement de DonnÃ©es dans une Base DuckDB :** Les fichiers CSV, Excel, JSON, et PDF peuvent Ãªtre chargÃ©s dans DuckDB.
5. **GÃ©nÃ©ration Automatique de RÃ©ponses et d'Outils :** Utilisation de modÃ¨les LLM (Ã  partir de LangChain) pour gÃ©nÃ©rer des plans d'action, des requÃªtes SQL et des analyses complÃ¨tes.

## Organisation des Fichiers

- `main.py` : Le script principal pour exÃ©cuter l'application.
- `requirements.txt` : Liste des dÃ©pendances Ã  installer.
- `README.md` : Ce fichier de documentation.
- `CHANGELOG.md` : Ce fichier rÃ©capitulatif des diffÃ©rentes versions du projet.
- `src/`: Ce dossier contient les fichiers sources du projet. 
- `pipelines/`: Ce dossier contient le script python de la sous solution sous forme de pipeline OpenWebui.
- `.env`: Ce fichier contient les variables d'environnement essentielles pour une bonne exÃ©cution.
- `docker-compose.yml`: Ce script Docker va permettre de mettre en place l'interface OpenWEBUI ainsi que son serveur Pipeline
- `run_pipelines.sh`: Ce script sh va permettre de lancer l'interface OpenWEBUI.
- `run_terminal.sh`: Ce script va permettre de lancer le data interpreter en mode terminal.
- `version.py`: Ce fichier python rÃ©fÃ©rence la version actuelle de la solution dÃ©ployÃ©e.

## Exemples de Fichiers SupportÃ©s

- **Excel (.xls, .xlsx, xlsm)** : Chargement de toutes les feuilles disponibles dans une base de donnÃ©es.
- **CSV (.csv)** : Chargement dans une table DuckDB avec traitement prÃ©alable.
- **JSON (.json)** : Normalisation des donnÃ©es imbriquÃ©es et chargement.
- **PDF (.pdf)** : Extraction de texte et images avec OCR.
- **Python (.py)** : Analyse et extraction du code, des fonctions, classes, et autres Ã©lÃ©ments Python.

## Exemples d'usage avancÃ©s

### 1. Analyse de donnÃ©es de ventes (Excel/CSV)

**Fichier :** `ventes_2024.xlsx`
```json
{
  "complex_query": "Analyse les tendances de ventes par mois et identifie les produits les plus performants. CrÃ©e un graphique des ventes mensuelles."
}
```

**RÃ©sultat attendu :**
- Analyse statistique des ventes
- Identification des top produits
- GÃ©nÃ©ration d'un graphique matplotlib
- Recommandations basÃ©es sur les tendances

### 2. Extraction et analyse de documents PDF

**Fichier :** `rapport_financier.pdf`
```json
{
  "complex_query": "Extrait tous les tableaux financiers du PDF et calcule les ratios de rentabilitÃ©. Compare avec les annÃ©es prÃ©cÃ©dentes si disponible."
}
```

**CapacitÃ©s :**
- OCR automatique des tableaux
- Extraction de donnÃ©es numÃ©riques
- Calculs financiers automatisÃ©s
- Analyse comparative

### 3. Analyse de code Python

**Fichier :** `mon_projet.py`
```json
{
  "complex_query": "Analyse la complexitÃ© du code, identifie les fonctions non utilisÃ©es et suggÃ¨re des amÃ©liorations de performance."
}
```

**FonctionnalitÃ©s :**
- Analyse AST (Abstract Syntax Tree)
- DÃ©tection de code mort
- MÃ©triques de complexitÃ©
- Suggestions d'optimisation

### 4. Traitement de donnÃ©es JSON complexes

**Fichier :** `api_logs.json`
```json
{
  "complex_query": "Analyse les logs d'API, identifie les endpoints les plus utilisÃ©s et dÃ©tecte les anomalies de performance."
}
```

**Traitement :**
- Normalisation de JSON imbriquÃ©
- Analyse temporelle
- DÃ©tection d'anomalies
- Visualisation des patterns

### 5. Analyse multi-fichiers avec corrÃ©lations

**Fichiers :** `ventes.csv`, `marketing.xlsx`, `feedback.json`
```json
{
  "complex_query": "CorrÃ¨le les donnÃ©es de ventes avec les campagnes marketing et le feedback client. Identifie quelles campagnes ont le meilleur ROI."
}
```

**Analyse croisÃ©e :**
- Jointures automatiques entre sources
- Calculs de corrÃ©lation
- Analyse de ROI
- Recommandations stratÃ©giques

### 6. Utilisation des flags avancÃ©s

**Forcer une nouvelle analyse :**
```json
{
  "complex_query": "Refais l'analyse des ventes avec les nouveaux paramÃ¨tres #force"
}
```

**Mode consultation rapide :**
```json
{
  "complex_query": "Explique-moi ce qu'est un ratio de liquiditÃ© #pass"
}
```

### 7. GÃ©nÃ©ration de rapports automatisÃ©s

**RequÃªte complexe :**
```json
{
  "complex_query": "GÃ©nÃ¨re un rapport exÃ©cutif complet incluant : 1) RÃ©sumÃ© des KPIs, 2) Analyse des tendances, 3) Graphiques de performance, 4) Recommandations actionables. Sauvegarde le tout en PDF."
}
```

**Sortie :**
- Rapport PDF structurÃ©
- Graphiques intÃ©grÃ©s
- Tableaux de donnÃ©es
- Recommandations personnalisÃ©es

### 8. Analyse prÃ©dictive

**DonnÃ©es historiques :**
```json
{
  "complex_query": "Utilise les donnÃ©es des 3 derniÃ¨res annÃ©es pour prÃ©dire les ventes du prochain trimestre. Inclus un intervalle de confiance."
}
```

**ModÃ©lisation :**
- RÃ©gression temporelle
- Analyse de saisonnalitÃ©
- PrÃ©dictions avec intervalles
- Validation croisÃ©e

## Exemple de commandes additionnelles

- `sh run_terminal.sh --help` : Permet d'obtenir le helper du programme
- `sh run_terminal.sh --v` : Permet d'obtenir le numÃ©ro de version du programme actuel

## Flags Ã  inclure si besoin dans les requÃªtes

- `#force` : Permet de forcer le traitement d'une question meme si elle a Ã©tÃ© traitÃ©e prÃ©cÃ©demment.
- `#pass` : Permet de ne pas utiliser le processus de traitement et de questionner simplement le LLM.

### Exemple de requÃªte avec le flag #force

```
{
  "complex_query": "Quel est le nom de la premiÃ¨re colonne ? #force"
}
```

### Exemple de requÃªte avec le flag #pass

```
{
  "complex_query": "Quel est le nom de la premiÃ¨re colonne ? #pass"
}
```

## Configuration de l'environnement

Ã€ la racine du projet, vous allez pouvoir retrouver un fichier `.env` qui vous permettra de configurer les diffÃ©rents paramÃ¨tres du projet. Ce fichier contient toutes les variables d'environnement nÃ©cessaires au bon fonctionnement de l'application en mode Terminal et en mode Pipeline (OpenWebUI).

### Variables de configuration principales

#### Configuration du serveur (Mode Terminal)
- `PORT` : Port d'Ã©coute du serveur (par dÃ©faut : 8000)
- `ADDRESS` : Adresse IP d'Ã©coute (par dÃ©faut : 0.0.0.0)

#### ModÃ¨les LLM
- `DATABASE_MODEL` : ModÃ¨le utilisÃ© pour les requÃªtes de base de donnÃ©es (ex: duckdb-nsql:latest)
- `REASONING_MODEL` : ModÃ¨le utilisÃ© pour le raisonnement et l'analyse (ex: mistral-small3.1:latest)
- `PLAN_MODEL` : ModÃ¨le utilisÃ© pour la planification des tÃ¢ches (ex: mistral-small3.1:latest)
- `CODE_MODEL` : ModÃ¨le utilisÃ© pour la gÃ©nÃ©ration de code (ex: mistral-small3.1:latest)

#### Base de donnÃ©es
- `DB_FILE` : Chemin vers la base de donnÃ©es DuckDB principale
  - Mode Pipeline : `/srv/data/id/my_database.duckdb`
  - Mode Terminal : `./db/my_database.duckdb`
- `HISTORY_DB_FILE` : Chemin vers la base de donnÃ©es d'historique des conversations
  - Mode Pipeline : `/srv/data/id/chat_history.duckdb`
  - Mode Terminal : `./db/chat_history.duckdb`

#### Agent Python
- `SAVE_DIRECTORY` : RÃ©pertoire de sauvegarde des fichiers gÃ©nÃ©rÃ©s
  - Mode Pipeline : `/srv/`
  - Mode Terminal : `./output`

#### Configuration des logs
Chaque module possÃ¨de ses propres paramÃ¨tres de logging :

**Fichier principal (main.py)**
- `LOG_FILE_main` : Chemin du fichier de log
- `LOG_LEVEL_main` : Niveau de log (DEBUG, INFO, WARNING, ERROR)

**Autres modules** (history_func, LlmGeneration, PdfExtension, PythonExtension, PythonTool, SetupDatabase, SqlTool)
- `LOG_FILE_[module]` : Chemin du fichier de log pour chaque module
- `LOG_LEVEL_[module]` : Niveau de log pour chaque module

#### Configuration Ollama
- `OLLAMA_URL` : URL de l'API Ollama pour les embeddings
  - Mode Pipeline : `http://host.docker.internal:11434/api/embeddings`
  - Mode Terminal : `http://localhost:11434/api/embeddings`

#### Configuration Pipeline (OpenWebUI)
- `DATA_DIRECTORY` : RÃ©pertoire des donnÃ©es pour le mode Pipeline
- `OPENWEBUI_API_KEY` : ClÃ© API pour OpenWebUI
- `OPENWEBUI_API` : URL de l'API OpenWebUI
- `DOWNLOAD_URL` : URL de tÃ©lÃ©chargement des fichiers

### Modes de fonctionnement

Le fichier `.env` est configurÃ© pour supporter deux modes :
- **Mode Terminal** : Utilise des chemins locaux relatifs (commentÃ©s par dÃ©faut)
- **Mode Pipeline** : Utilise des chemins Docker pour l'intÃ©gration OpenWebUI (actifs par dÃ©faut)

Pour basculer entre les modes, commentez/dÃ©commentez les lignes appropriÃ©es dans le fichier `.env`.

Ces paramÃ¨tres peuvent Ãªtre modifiÃ©s selon vos besoins et votre environnement de dÃ©ploiement.

## Lancement Interface OpenWebui

Pour lancer l'interface OpenWebui, exÃ©cutez la commande suivante :

```bash
sh run_pipelines.sh
```

## Les commandes utiles Ã  lancer dans le mode terminal

- Afin d'avoir une documentation dÃ©taillÃ©e, vous pouvez faire cette commande help au dÃ©marrage.
```bash
sh run_terminal.sh --help
```

- Afin de connaitre la version actuelle de la solution : 
```bash
sh run_terminal.sh --v
```

## FAQ

### Questions frÃ©quentes

#### ğŸš€ **Installation et Configuration**

**Q: Quels sont les prÃ©requis systÃ¨me minimum ?**
R: Python 3.11+, 8GB RAM recommandÃ©s, 10GB d'espace disque libre pour les modÃ¨les Ollama.

**Q: Pourquoi Ollama ne se connecte pas ?**
R: VÃ©rifiez que le service Ollama est dÃ©marrÃ© (`ollama serve`) et que les modÃ¨les sont tÃ©lÃ©chargÃ©s. En mode Docker, utilisez `host.docker.internal:11434`.

**Q: Comment changer les modÃ¨les LLM utilisÃ©s ?**
R: Modifiez les variables dans le fichier `.env` : `DATABASE_MODEL`, `REASONING_MODEL`, `PLAN_MODEL`, `CODE_MODEL`.

#### ğŸ“ **Gestion des fichiers**

**Q: Quels formats de fichiers sont supportÃ©s ?**
R: Excel (.xls, .xlsx, .xlsm), CSV, JSON, PDF, Python (.py). Voir la section [Exemples de Fichiers SupportÃ©s](#exemples-de-fichiers-supportÃ©s).

**Q: Quelle est la taille maximum des fichiers ?**
R: Pas de limite stricte, mais les performances peuvent Ãªtre affectÃ©es au-delÃ  de 100MB. Pour les gros fichiers, utilisez le mode streaming.

**Q: Comment traiter plusieurs fichiers simultanÃ©ment ?**
R: Placez tous les fichiers dans le mÃªme dossier et lancez l'application. Le systÃ¨me dÃ©tectera automatiquement les relations entre fichiers.

#### ğŸ”§ **Utilisation et API**

**Q: Comment utiliser les flags #force et #pass ?**
R: 
- `#force` : Force une nouvelle analyse mÃªme si la question a dÃ©jÃ  Ã©tÃ© traitÃ©e
- `#pass` : Interroge directement le LLM sans traitement des donnÃ©es

**Q: L'API REST ne rÃ©pond pas, que faire ?**
R: VÃ©rifiez les logs dans `./Logs/` ou `/app/Logs/` selon le mode. Assurez-vous que le port 8000 n'est pas utilisÃ©.

**Q: Comment accÃ©der Ã  l'interface OpenWebUI ?**
R: AprÃ¨s `sh run_pipelines.sh`, accÃ©dez Ã  `http://localhost:3001`. L'interface peut prendre quelques minutes Ã  dÃ©marrer.

#### ğŸ› **RÃ©solution de problÃ¨mes**

**Q: Erreur "Extension sqlite_scanner not found" ?**
R: Cette erreur a Ã©tÃ© corrigÃ©e dans les versions rÃ©centes. Assurez-vous d'utiliser la derniÃ¨re version du code.

**Q: Les graphiques ne s'affichent pas ?**
R: VÃ©rifiez que matplotlib est installÃ© (`pip install matplotlib`) et que le rÃ©pertoire de sortie est accessible.

**Q: Erreur de mÃ©moire lors du traitement ?**
R: RÃ©duisez la taille des fichiers ou augmentez la RAM disponible. Utilisez le mode streaming pour les gros datasets.

#### ğŸ”’ **SÃ©curitÃ© et Performance**

**Q: Les donnÃ©es sont-elles sÃ©curisÃ©es ?**
R: Les donnÃ©es restent locales. En mode Pipeline, elles sont isolÃ©es par chat_id. Ã‰vitez de traiter des donnÃ©es sensibles en production.

**Q: Comment amÃ©liorer les performances ?**
R: 
- Utilisez un SSD pour les bases de donnÃ©es
- Augmentez la RAM disponible
- Utilisez des modÃ¨les LLM plus petits si nÃ©cessaire
- Activez le cache des conversations

**Q: Puis-je utiliser d'autres modÃ¨les LLM ?**
R: Oui, tout modÃ¨le compatible Ollama peut Ãªtre utilisÃ©. Modifiez les variables d'environnement correspondantes.

#### ğŸ”„ **Modes de fonctionnement**

**Q: Quelle est la diffÃ©rence entre mode Terminal et Pipeline ?**
R: 
- **Terminal** : Interface en ligne de commande, fichiers locaux
- **Pipeline** : Interface web OpenWebUI, environnement Docker

**Q: Comment basculer entre les modes ?**
R: Modifiez les commentaires dans le fichier `.env` pour activer/dÃ©sactiver les configurations appropriÃ©es.

**Q: Puis-je utiliser les deux modes simultanÃ©ment ?**
R: Non recommandÃ©, car ils utilisent des chemins de base de donnÃ©es diffÃ©rents. Choisissez un mode principal.

#### ğŸ“Š **Analyse de donnÃ©es**

**Q: Comment interprÃ©ter les rÃ©sultats d'analyse ?**
R: Le systÃ¨me gÃ©nÃ¨re des explications en langage naturel. Pour plus de dÃ©tails, demandez des clarifications avec des questions de suivi.

**Q: Les prÃ©dictions sont-elles fiables ?**
R: Les prÃ©dictions sont basÃ©es sur les donnÃ©es historiques et des modÃ¨les statistiques. Toujours valider avec votre expertise mÃ©tier.

**Q: Comment sauvegarder les analyses ?**
R: Les rÃ©sultats sont automatiquement sauvegardÃ©s dans le rÃ©pertoire configurÃ© (`SAVE_DIRECTORY`). L'historique est conservÃ© dans la base de donnÃ©es.

## Avertissement

Ce projet est en cours de dÃ©veloppement et peut contenir des bugs. Utilisez-le Ã  vos risques et pÃ©rils. Les rÃ©sultats gÃ©nÃ©rÃ©s par le LLM peuvent ne pas Ãªtre exacts ou appropriÃ©s pour toutes les situations. Veuillez toujours vÃ©rifier les rÃ©sultats avant de les utiliser dans un contexte critique.
